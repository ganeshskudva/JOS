/* See COPYRIGHT for copyright information. */
/* 
 * =============================================
 *  Name             Date        Changes
 * ==============================================
 *  Manikantan S        09/17/11  Lab2
 *  Manikantan S/Ganesh 09/29/11  Lab3
 */


#include <inc/x86.h>
#include <inc/mmu.h>
#include <inc/error.h>
#include <inc/string.h>
#include <inc/assert.h>

#include <kern/pmap.h>
#include <kern/kclock.h>
#include <kern/env.h>
#include <kern/e100.h>

// These variables are set by i386_detect_memory()
static physaddr_t maxpa;	// Maximum physical address
size_t npage;			// Amount of physical memory (in pages)
static size_t basemem;		// Amount of base memory (in bytes)
static size_t extmem;		// Amount of extended memory (in bytes)

// These variables are set in i386_vm_init()
pde_t* boot_pgdir;		// Virtual address of boot time page directory
physaddr_t boot_cr3;		// Physical address of boot time page directory
char* boot_freemem;	// Pointer to next byte of free mem  TODO  : changed static 

struct Page* pages;		// Virtual address of physical page array
static struct Page_list page_free_list;	// Free list of physical pages

static uint16_t check_if_page_is_used(uint16_t phys_page_num);
static uint16_t is_page_lowmem( uint16_t pagenum);
static uint16_t is_page_io_hole( uint16_t pagenum);
static int mapva2pa(pde_t *pgdir, struct Page *pp, void *va, int perm) ;
static void dump_directory_content(pde_t *pgdir);



// Global descriptor table.
//
// The kernel and user segments are identical (except for the DPL).
// To load the SS register, the CPL must equal the DPL.  Thus,
// we must duplicate the segments for the user and the kernel.
//
struct Segdesc gdt[] =
{
	// 0x0 - unused (always faults -- for trapping NULL far pointers)
	SEG_NULL,

	// 0x8 - kernel code segment
	[GD_KT >> 3] = SEG(STA_X | STA_R, 0x0, 0xffffffff, 0),

	// 0x10 - kernel data segment
	[GD_KD >> 3] = SEG(STA_W, 0x0, 0xffffffff, 0),

	// 0x18 - user code segment
	[GD_UT >> 3] = SEG(STA_X | STA_R, 0x0, 0xffffffff, 3),

	// 0x20 - user data segment
	[GD_UD >> 3] = SEG(STA_W, 0x0, 0xffffffff, 3),

	// 0x28 - tss, initialized in idt_init()
	[GD_TSS >> 3] = SEG_NULL
};

struct Pseudodesc gdt_pd = {
	sizeof(gdt) - 1, (unsigned long) gdt
};

static int
nvram_read(int r)
{
	return mc146818_read(r) | (mc146818_read(r + 1) << 8);
}

void
i386_detect_memory(void)
{
	// CMOS tells us how many kilobytes there are
	basemem = ROUNDDOWN(nvram_read(NVRAM_BASELO)*1024, PGSIZE);
	extmem = ROUNDDOWN(nvram_read(NVRAM_EXTLO)*1024, PGSIZE);

	// Calculate the maximum physical address based on whether
	// or not there is any extended memory.  See comment in <inc/mmu.h>.
	if (extmem)
		maxpa = EXTPHYSMEM + extmem;
	else
		maxpa = basemem;

	npage = maxpa / PGSIZE;

	cprintf("Physical memory: %dK available, ", (int)(maxpa/1024));
	cprintf("base = %dK, extended = %dK\n", (int)(basemem/1024), (int)(extmem/1024));
        cprintf("number of pages %d\n",npage);
}

// --------------------------------------------------------------
// Set up initial memory mappings and turn on MMU.
// --------------------------------------------------------------

static void check_boot_pgdir(void);
static void check_page_alloc();
static void page_check(void);
static void boot_map_segment(pde_t *pgdir, uintptr_t la, size_t size, physaddr_t pa, int perm);

//
// A simple physical memory allocator, used only a few times
// in the process of setting up the virtual memory system.
// page_alloc() is the real allocator.
//
// Allocate n bytes of physical memory aligned on an 
// align-byte boundary.  Align must be a power of two.
// Return kernel virtual address.  Returned memory is uninitialized.
//
// If we're out of memory, boot_alloc should panic.
// This function may ONLY be used during initialization,
// before the page_free_list has been set up.
// 
static void*
boot_alloc(uint32_t n, uint32_t align)
{
	extern char end[];
	void *v;

	// Initialize boot_freemem if this is the first time.
	// 'end' is a magic symbol automatically generated by the linker,
	// which points to the end of the kernel's bss segment -
	// i.e., the first virtual address that the linker
	// did _not_ assign to any kernel code or global variables.
	if (boot_freemem == 0)
		boot_freemem = end;

         //Print the end address.
         //cprintf("MANI end is %u\n",end);

	// LAB 2: Your code here:
	//	Step 1: round boot_freemem up to be aligned properly
	//		(hint: look in types.h for some handy macros)
	//	Step 2: save current value of boot_freemem as allocated chunk
	//	Step 3: increase boot_freemem to record allocation
	//	Step 4: return allocated chunk

         v = ROUNDUP( boot_freemem,align);

         //Update the boot_freemem to point to the next available
         // Memory. boot_alloc will be called one more time to get the 
         // memory needed to store the pageinfo_ds  
         boot_freemem = v + n;

         //Print the end address.
         //cprintf("\nMANI end is %x boot %x virt %x \n",end,boot_freemem,virtual_address_pgdir);
	//return virtual_address_pgdir;
        return  v;
}

// Set up a two-level page table:
//    boot_pgdir is its linear (virtual) address of the root
//    boot_cr3 is the physical adresss of the root
// Then turn on paging.  Then effectively turn off segmentation.
// (i.e., the segment base addrs are set to zero).
// 
// This function only sets up the kernel part of the address space
// (ie. addresses >= UTOP).  The user part of the address space
// will be setup later.
//
// From UTOP to ULIM, the user is allowed to read but not write.
// Above ULIM the user cannot read (or write). 
void
i386_vm_init(void)
{
	pde_t* pgdir;
	uint32_t cr0;
	size_t n;
	extern char end[];

	// Delete this line:
	//panic("i386_vm_init: This function is not finished\n");

	//////////////////////////////////////////////////////////////////////
	// create initial page directory.
	pgdir = boot_alloc(PGSIZE, PGSIZE);
	memset(pgdir, 0, PGSIZE);
	boot_pgdir = pgdir;
	boot_cr3 = PADDR(pgdir);

        //cprintf("MANI  - is%x %x\n",boot_pgdir,end);
        //cprintf("MANI VPT is %x PDX(VPT) %x \n",VPT,PDX(VPT));
        //cprintf("MANI UVPT is %x PDX(UVPT) %x \n",UVPT,PDX(UVPT));
	//////////////////////////////////////////////////////////////////////
	// Recursively insert PD in itself as a page table, to form
	// a virtual page table at virtual address VPT.
	// (For now, you don't have understand the greater purpose of the
	// following two lines.)

	// Permissions: kernel RW, user NONE
	pgdir[PDX(VPT)] = PADDR(pgdir)|PTE_W|PTE_P;

	// same for UVPT
	// Permissions: kernel R, user R 
	pgdir[PDX(UVPT)] = PADDR(pgdir)|PTE_U|PTE_P;

	//////////////////////////////////////////////////////////////////////
	// Allocate an array of npage 'struct Page's and store it in 'pages'.
	// The kernel uses this array to keep track of physical pages: for
	// each physical page, there is a corresponding struct Page in this
	// array.  'npage' is the number of physical pages in memory.
	// User-level programs will get read-only access to the array as well.
	// Your code goes here:
 
        uint32_t pageinfo_ds_size;
        pageinfo_ds_size = npage * sizeof( struct Page);
        //cprintf("MANI- Number of pages %d,Data structure size %d\n",
        //         npage, pageinfo_ds_size);
        pages = boot_alloc ( pageinfo_ds_size,PGSIZE);

        //intialiize the Page Information Data strcuture 
        memset(pages,0x00,pageinfo_ds_size); 
        //cprintf("MANI-  my pages struct starts at %x %x\n",pages,PADDR(pages));


	//////////////////////////////////////////////////////////////////////
	// Make 'envs' point to an array of size 'NENV' of 'struct Env'.
	// LAB 3: Your code here.

       uint32_t envinfo_ds_size ;
       envinfo_ds_size = NENV * sizeof ( struct Env);
       envs = boot_alloc( envinfo_ds_size, PGSIZE );
       memset( envs,0x00,envinfo_ds_size);

       //Mark contiguous block of pages for NIC's Tx Buffer space
       e100_tx_buff = (uint32_t) boot_alloc(E100_MAX_TX_BUFFERS*PGSIZE,PGSIZE);
       memset((void*)e100_tx_buff,0x00,E100_MAX_TX_BUFFERS * PGSIZE);

       //Mark contiguous block of pages for NIC's Rx Buffer space
       e100_rx_buff = (uint32_t) boot_alloc(E100_MAX_RX_BUFFERS*PGSIZE,PGSIZE);
       memset((void*)e100_rx_buff,0x00,E100_MAX_RX_BUFFERS * PGSIZE);

       cprintf("BOOTFREEMEM : %x\n",boot_freemem);
	//////////////////////////////////////////////////////////////////////
	// Now that we've allocated the initial kernel data structures, we set
	// up the list of free physical pages. Once we've done so, all further
	// memory management will go through the page_* functions. In
	// particular, we can now map memory using boot_map_segment or page_insert
	page_init();

	check_page_alloc();

	page_check();


	//////////////////////////////////////////////////////////////////////
	// Now we set up virtual memory 
	
	//////////////////////////////////////////////////////////////////////
	// Map 'pages' read-only by the user at linear address UPAGES
	// Permissions:
	//    - the new image at UPAGES -- kernel R, user R
	//      (ie. perm = PTE_U | PTE_P)
	//    - pages itself -- kernel RW, user NONE
	// Your code goes here:

        //Determine the size of pages array 
        uint32_t page_array_len = npage * sizeof(struct Page);
        page_array_len = ROUNDUP(page_array_len,PGSIZE);
        boot_map_segment(pgdir,UPAGES,page_array_len,PADDR(pages),(PTE_P|PTE_U));
        boot_map_segment(pgdir,(uintptr_t)pages,page_array_len,PADDR(pages),(PTE_P|PTE_W));


	//////////////////////////////////////////////////////////////////////
	// Map the 'envs' array read-only by the user at linear address UENVS
	// (ie. perm = PTE_U | PTE_P).
	// Permissions:
	//    - the new image at UENVS  -- kernel R, user R
	//    - envs itself -- kernel RW, user NONE
	// LAB 3: Your code here.
        
        uint32_t env_array_len = NENV * sizeof(struct Env);
        env_array_len = ROUNDUP(env_array_len,PGSIZE);
        boot_map_segment(pgdir,UENVS,env_array_len,PADDR(envs),(PTE_P|PTE_U));
        boot_map_segment(pgdir,(uintptr_t)envs,env_array_len,PADDR(envs),(PTE_P|PTE_W));

	//////////////////////////////////////////////////////////////////////
	// Use the physical memory that 'bootstack' refers to as the kernel
	// stack.  The kernel stack grows down from virtual address KSTACKTOP.
	// We consider the entire range from [KSTACKTOP-PTSIZE, KSTACKTOP) 
	// to be the kernel stack, but break this into two pieces:
	//     * [KSTACKTOP-KSTKSIZE, KSTACKTOP) -- backed by physical memory
	//     * [KSTACKTOP-PTSIZE, KSTACKTOP-KSTKSIZE) -- not backed; so if
	//       the kernel overflows its stack, it will fault rather than
	//       overwrite memory.  Known as a "guard page".
	//     Permissions: kernel RW, user NONE
	// Your code goes here:
        boot_map_segment(pgdir,KSTACKTOP-KSTKSIZE,KSTKSIZE,PADDR(bootstack),( PTE_P | PTE_W) );

	//////////////////////////////////////////////////////////////////////
	// Map all of physical memory at KERNBASE. 
	// Ie.  the VA range [KERNBASE, 2^32) should map to
	//      the PA range [0, 2^32 - KERNBASE)
	// We might not have 2^32 - KERNBASE bytes of physical memory, but
	// we just set up the mapping anyway.
	// Permissions: kernel RW, user NONE
	// Your code goes here: 2^32 - KERNBASE = 256MB
        boot_map_segment(pgdir,KERNBASE,0x10000000,0,(PTE_P | PTE_W));


        //dump_directory_content(pgdir);

	// Check that the initial page directory has been set up correctly.
	check_boot_pgdir();


	//////////////////////////////////////////////////////////////////////
	// On x86, segmentation maps a VA to a LA (linear addr) and
	// paging maps the LA to a PA.  I.e. VA => LA => PA.  If paging is
	// turned off the LA is used as the PA.  Note: there is no way to
	// turn off segmentation.  The closest thing is to set the base
	// address to 0, so the VA => LA mapping is the identity.

	// Current mapping: VA KERNBASE+x => PA x.
	//     (segmentation base=-KERNBASE and paging is off)

	// From here on down we must maintain this VA KERNBASE + x => PA x
	// mapping, even though we are turning on paging and reconfiguring
	// segmentation.

	// Map VA 0:4MB same as VA KERNBASE, i.e. to PA 0:4MB.
	// (Limits our kernel to <4MB)
	pgdir[0] = pgdir[PDX(KERNBASE)];

	// Install page table.
	lcr3(boot_cr3);

	// Turn on paging.
	cr0 = rcr0();
	cr0 |= CR0_PE|CR0_PG|CR0_AM|CR0_WP|CR0_NE|CR0_TS|CR0_EM|CR0_MP;
	cr0 &= ~(CR0_TS|CR0_EM);
	lcr0(cr0);

	// Current mapping: KERNBASE+x => x => x.
	// (x < 4MB so uses paging pgdir[0])

	// Reload all segment registers.
	asm volatile("lgdt gdt_pd");
	asm volatile("movw %%ax,%%gs" :: "a" (GD_UD|3));
	asm volatile("movw %%ax,%%fs" :: "a" (GD_UD|3));
	asm volatile("movw %%ax,%%es" :: "a" (GD_KD));
	asm volatile("movw %%ax,%%ds" :: "a" (GD_KD));
	asm volatile("movw %%ax,%%ss" :: "a" (GD_KD));
	asm volatile("ljmp %0,$1f\n 1:\n" :: "i" (GD_KT));  // reload cs
	asm volatile("lldt %%ax" :: "a" (0));

	// Final mapping: KERNBASE+x => KERNBASE+x => x.

	// This mapping was only used after paging was turned on but
	// before the segment registers were reloaded.
	pgdir[0] = 0;

	// Flush the TLB for good measure, to kill the pgdir[0] mapping.
	lcr3(boot_cr3);

        cprintf("BOoTFREEMEM : %x\n",boot_freemem);

}

static void dump_directory_content(pde_t *pgdir)
{
    int i=0;
    for(i=0;i<1024;i++)
    {
       cprintf("Index %d Adress %x\n",i,pgdir[i]);
    }
}
//
// Check the physical page allocator (page_alloc(), page_free(),
// and page_init()).
//
static void
check_page_alloc()
{
	struct Page *pp, *pp0, *pp1, *pp2;
	struct Page_list fl;

	// if there's a page that shouldn't be on
	// the free list, try to make sure it
	// eventually causes trouble.
	LIST_FOREACH(pp0, &page_free_list, pp_link)
		memset(page2kva(pp0), 0x97, 128);

	LIST_FOREACH(pp0, &page_free_list, pp_link) {
		// check that we didn't corrupt the free list itself
		assert(pp0 >= pages);
		assert(pp0 < pages + npage);

		// check a few pages that shouldn't be on the free list
		assert(page2pa(pp0) != 0);
		assert(page2pa(pp0) != IOPHYSMEM);
		assert(page2pa(pp0) != EXTPHYSMEM - PGSIZE);
		assert(page2pa(pp0) != EXTPHYSMEM);
		assert(page2kva(pp0) != ROUNDDOWN(boot_freemem - 1, PGSIZE));
	}

	// should be able to allocate three pages
	pp0 = pp1 = pp2 = 0;
	assert(page_alloc(&pp0) == 0);
	assert(page_alloc(&pp1) == 0);
	assert(page_alloc(&pp2) == 0);

	assert(pp0);
	assert(pp1 && pp1 != pp0);
	assert(pp2 && pp2 != pp1 && pp2 != pp0);
	assert(page2pa(pp0) < npage*PGSIZE);
	assert(page2pa(pp1) < npage*PGSIZE);
	assert(page2pa(pp2) < npage*PGSIZE);

	// temporarily steal the rest of the free pages
	fl = page_free_list;
	LIST_INIT(&page_free_list);

	// should be no free memory
	assert(page_alloc(&pp) == -E_NO_MEM);

	// free and re-allocate?
	page_free(pp0);
	page_free(pp1);
	page_free(pp2);
	pp0 = pp1 = pp2 = 0;
	assert(page_alloc(&pp0) == 0);
	assert(page_alloc(&pp1) == 0);
	assert(page_alloc(&pp2) == 0);
	assert(pp0);
	assert(pp1 && pp1 != pp0);
	assert(pp2 && pp2 != pp1 && pp2 != pp0);
	assert(page_alloc(&pp) == -E_NO_MEM);

	// give free list back
	page_free_list = fl;

	// free the pages we took
	page_free(pp0);
	page_free(pp1);
	page_free(pp2);

	cprintf("check_page_alloc() succeeded!\n");
}

//
// Checks that the kernel part of virtual address space
// has been setup roughly correctly(by i386_vm_init()).
//
// This function doesn't test every corner case,
// in fact it doesn't test the permission bits at all,
// but it is a pretty good sanity check. 
//
static physaddr_t check_va2pa(pde_t *pgdir, uintptr_t va);

static void
check_boot_pgdir(void)
{
	uint32_t i, n;
	pde_t *pgdir;

	pgdir = boot_pgdir;

	// check pages array
	n = ROUNDUP(npage*sizeof(struct Page), PGSIZE);
	for (i = 0; i < n; i += PGSIZE)
		assert(check_va2pa(pgdir, UPAGES + i) == PADDR(pages) + i);
	
	// check envs array (new test for lab 3)
	n = ROUNDUP(NENV*sizeof(struct Env), PGSIZE);
	for (i = 0; i < n; i += PGSIZE)
		assert(check_va2pa(pgdir, UENVS + i) == PADDR(envs) + i);

	// check phys mem
	for (i = 0; i < npage * PGSIZE; i += PGSIZE)
		assert(check_va2pa(pgdir, KERNBASE + i) == i);

	// check kernel stack
	for (i = 0; i < KSTKSIZE; i += PGSIZE)
		assert(check_va2pa(pgdir, KSTACKTOP - KSTKSIZE + i) == PADDR(bootstack) + i);
	assert(check_va2pa(pgdir, KSTACKTOP - PTSIZE) == ~0);

	// check for zero/non-zero in PDEs
	for (i = 0; i < NPDENTRIES; i++) {
		switch (i) {
		case PDX(VPT):
		case PDX(UVPT):
		case PDX(KSTACKTOP-1):
		case PDX(UPAGES):
		case PDX(UENVS):
			assert(pgdir[i]);
			break;
		default:
			if (i >= PDX(KERNBASE))
				assert(pgdir[i]);
			else
				assert(pgdir[i] == 0);
			break;
		}
	}
	cprintf("check_boot_pgdir() succeeded!\n");
}

// This function returns the physical address of the page containing 'va',
// defined by the page directory 'pgdir'.  The hardware normally performs
// this functionality for us!  We define our own version to help check
// the check_boot_pgdir() function; it shouldn't be used elsewhere.

static physaddr_t
check_va2pa(pde_t *pgdir, uintptr_t va)
{
	pte_t *p;

	pgdir = &pgdir[PDX(va)];
	if (!(*pgdir & PTE_P))
		return ~0;
	p = (pte_t*) KADDR(PTE_ADDR(*pgdir));
	if (!(p[PTX(va)] & PTE_P))
		return ~0;
	return PTE_ADDR(p[PTX(va)]);
}
		
// --------------------------------------------------------------
// Tracking of physical pages.
// The 'pages' array has one 'struct Page' entry per physical page.
// Pages are reference counted, and free pages are kept on a linked list.
// --------------------------------------------------------------

//
// Initialize page structure and memory free list.
// After this is done, NEVER use boot_alloc again.  ONLY use the page
// allocator functions below to allocate and deallocate physical
// memory via the page_free_list.
//
void
page_init(void)
{
	// The example code here marks all physical pages as free.
	// However this is not truly the case.  What memory is free?
	//  1) Mark physical page 0 as in use.
	//     This way we preserve the real-mode IDT and BIOS structures
	//     in case we ever need them.  (Currently we don't, but...)
	//  2) The rest of base memory, [PGSIZE, basemem) is free.
	//  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM).
	//     Mark it as in use so that it can never be allocated.
	//  4) Then extended memory [EXTPHYSMEM, ...).
	//     Some of it is in use, some is free. Where is the kernel
	//     in physical memory?  Which pages are already in use for
	//     page tables and other data structures?
	//
	// Change the code to reflect this.

        int cnt=0;
	int i;
	LIST_INIT(&page_free_list);
	for (i = 0; i < npage; i++)
        {
              // Check if this page is Free or in-use.
              // We need the physical address of each page
              // to know, it is free or not . 
        
		pages[i].pp_ref = check_if_page_is_used(i);
                if ( pages[i].pp_ref == 0 )
                {
                   cnt++;
                   //cprintf("Mani Insert Page %d to list \n",i);
		   LIST_INSERT_HEAD(&page_free_list, &pages[i], pp_link);
                }
	}
        //cprintf("MANI - count is %d\n",cnt);
}

uint16_t is_page_between_kernbase_bootfreemem( uint16_t pagenum)
{
       //Pages including the  kernel code upto boot_freemem.
       uint16_t retval =0 ;

       //find the range of this given page.
       physaddr_t start_addr;
       physaddr_t end_addr;

       physaddr_t kern_start_address = PADDR ( KERNBASE );
       physaddr_t freemem = PADDR ( ROUNDUP(boot_freemem,PGSIZE) );

       start_addr = pagenum * PGSIZE;
       end_addr   = start_addr + PGSIZE - 1;

       if( ( start_addr >= kern_start_address ) &&
           ( end_addr < freemem )
         )
         {
               retval = 1;
         }

         return ( retval );
}

uint16_t is_page_io_hole( uint16_t pagenum)
{
    uint16_t retval = 0;

    //IO Hole is from  basmem to 1MB
    //find the range of this given page.
    physaddr_t start_addr;
    physaddr_t end_addr;
 
    start_addr = pagenum * PGSIZE;
    end_addr = start_addr + PGSIZE - 1;

    if( ( start_addr >= basemem ) &&
        ( end_addr < (1024*1024) )
      )
    {
         //cprintf("MANI - IO Hole Pagenum %d \n",pagenum);
         retval = 1;
    }

    return ( retval );
}

uint16_t is_page_lowmem( uint16_t pagenum)
{
    uint16_t retval = 0;

    //lowmem is from page-1 to basmem
    //find the range of this given page.
    physaddr_t start_addr;
    physaddr_t end_addr;
 
    start_addr = pagenum * PGSIZE;
    end_addr = start_addr + PGSIZE - 1;

    if( ( start_addr > PGSIZE - 1 ) &&
        ( end_addr < basemem)
      )
      {
         retval = 1;
         //cprintf( "MANI-LowMem Pagenum %d Used %d\n",pagenum,retval);
      }

     return( retval);
}
//This function checks if the given page is free or not
uint16_t check_if_page_is_used(uint16_t phys_page_num)
{
      uint16_t refcnt = 0;
 
        //0 to 4095 : Page zero: Used 
        //4096 to basmem : boot loader + misc : Free ==> Low Mem
        //basemem to 1MB : IO Hole: Used
        //1MB to END : Kernel code : Used
        //END to pages: Page Directory  : Used 
        //pages to envs : Pageinfostructure : Used.
        //envs to boot_freemem : Environment book keeping : Used
        //boot_freemen to maxpa : empty : Free

      if( phys_page_num == 0 ) 
      {
            refcnt = 1 ;
      }    
      else
      {
           if( is_page_lowmem(phys_page_num) )
           {
               refcnt = 0;   //No Need to check , anyway lowmem is free
           }
           else if ( is_page_io_hole(phys_page_num) )
           {
                refcnt = 1;
           }
           else if ( is_page_between_kernbase_bootfreemem( phys_page_num) )
           {
                refcnt = 1;
           }
      }

      return( refcnt ) ;
}
//
// Initialize a Page structure.
// The result has null links and 0 refcount.
// Note that the corresponding physical page is NOT initialized!
//
static void
page_initpp(struct Page *pp)
{
	memset(pp, 0, sizeof(*pp));
}

//
// Allocates a physical page.
// Does NOT set the contents of the physical page to zero, NOR does it
// increment the reference count of the page - the caller must do
// these if necessary. 
//
// *pp_store -- is set to point to the Page struct of the newly allocated
// page
//
// RETURNS 
//   0 -- on success
//   -E_NO_MEM -- otherwise 
//
// Hint: use LIST_FIRST, LIST_REMOVE, and page_initpp
int
page_alloc(struct Page **pp_store)
{
        struct Page * elem_to_remove =  LIST_FIRST(&page_free_list);

        //see if there is a free page in the system
        if( elem_to_remove != NULL)
        {
             LIST_REMOVE(elem_to_remove,pp_link);
             *pp_store = elem_to_remove ;
              page_initpp(elem_to_remove);
             return ( 0) ;
        }

	return -E_NO_MEM;
}

//
// Return a page to the free list.
// (This function should only be called when pp->pp_ref reaches 0.)
//
void
page_free(struct Page *pp)
{
       if( pp != NULL)
       {
          LIST_INSERT_HEAD(&page_free_list, pp , pp_link);
       }
}

//
// Decrement the reference count on a page,
// freeing it if there are no more refs.
//
void
page_decref(struct Page* pp)
{
       
	if (--pp->pp_ref == 0)
         {
               //cprintf("MANI-DEBUG Reference for page %x hit 0 \n",pp - pages);
		page_free(pp);
          }
}


// Given 'pgdir', a pointer to a page directory, pgdir_walk returns
// a pointer to the page table entry (PTE) for linear address 'va'.
// This requires walking the two-level page table structure.
//
// If the relevant page table doesn't exist in the page directory, then:
//    - If create == 0, pgdir_walk returns NULL.
//    - Otherwise, pgdir_walk tries to allocate a new page table
//	with page_alloc.  If this fails, pgdir_walk returns NULL.
//    - pgdir_walk sets pp_ref to 1 for the new page table.
//    - pgdir_walk clears the new page table.
//    - Finally, pgdir_walk returns a pointer into the new page table.
//
// Hint: you can turn a Page * into the physical address of the
// page it refers to with page2pa() from kern/pmap.h.
//
// Hint 2: the x86 MMU checks permission bits in both the page directory
// and the page table, so it's safe to leave permissions in the page
// more permissive than strictly necessary.
pte_t *
pgdir_walk(pde_t *pgdir, const void *va, int create)
{
    // Fill this function in
    // First get the DIR ( 10 bits ) from the VA.

     uint16_t index = PDX(va);
     uint32_t dir_entry = pgdir[index];


     //Extract the Present bit to see if there is a valid Page Table for 
     // this VA.
     if ( ( dir_entry & PTE_P ) == 0 )
     {

         //cprintf("MANI - WALK syays, there is no page tbale for %d\n",va);
         //There is no Page Table for this VA
         if ( create == 0 )
         {
             //Bye
             return ( NULL );
         }
         else
         {
             //Create a Page Table and Update this dir_entry.
             int alloc_result = 0;     
             struct Page * page_table = 0x00;

             alloc_result  = page_alloc( &page_table );
             //Did we get a page ? 
             if(alloc_result ==0 )
             {
                //Good Proceed Further 
                //First clear the new page  table
                memset(page2kva(page_table),0x00,PGSIZE);

                //udpate the ref
                 page_table->pp_ref = 1;

                //Update the current dir_entry to point to this page
                //strip of last 000 from page2pa and store the PPN 
                 dir_entry = PTE_ADDR( page2pa( page_table) ) ;
                 dir_entry |= PTE_USER;
 
                 pgdir[PDX(va)] = dir_entry;

                 pte_t *kva =  page2kva(page_table);
                 tlb_invalidate(pgdir,(void*)va);

                 return ( & kva[PTX(va)] );
                
             }
             else
             {
                //cprintf("MANI CRITICAL - No Free Page to allocate Page table\n");
                 return( NULL );
             }
         }
     }
     else
     {
        //There is already a Page Table for this VA 
        //Get the VIRTUAL Adress of the corresponding PTE

        pte_t * p  = ( pte_t *) KADDR ( PTE_ADDR ( dir_entry ) ) ;
        //dir_entry |= create;
        //pgdir[PDX(va)] = dir_entry;
        return & p[ PTX(va)];
     }
    
	return NULL;
}

//
// Map the physical page 'pp' at virtual address 'va'.
// The permissions (the low 12 bits) of the page table
//  entry should be set to 'perm|PTE_P'.
//
// Requirements
//   - If there is already a page mapped at 'va', it should be page_remove()d.
//   - If necessary, on demand, a page table should be allocated and inserted
//     into 'pgdir'.
//   - pp->pp_ref should be incremented if the insertion succeeds.
//   - The TLB must be invalidated if a page was formerly present at 'va'.
//
// Corner-case hint: Make sure to consider what happens when the same 
// pp is re-inserted at the same virtual address in the same pgdir.
//
// RETURNS: 
//   0 on success
//   -E_NO_MEM, if page table couldn't be allocated
//
// Hint: The TA solution is implemented using pgdir_walk, page_remove,
// and page2pa.
//
int
page_insert(pde_t *pgdir, struct Page *pp, void *va, int perm) 
{
	// Fill this function in
        pte_t * ptr = NULL;
        int retval = -E_NO_MEM;


        //cprintf("MANI - Request to map %x at num %d\n",va,pp - pages);

        ptr = pgdir_walk(pgdir,va,0);
 
        //cprintf("Walk return %x\n",ptr);

        if ( ptr !=NULL )
        {
           // We have a Page table for this va , check it 
           // there is a valid PTE for this va if so  this va is already 
           // mapped to a PA . Remove the entry and re-map it to a provided 
           // PA

           //cprintf("MANI - there is a page table for %x\n",va);

           if ( ( *ptr & PTE_P ) == 1 )
           {
               //cprintf("MANI - there is a valid pte for %x\n",va);

               //Before removing the page which is mapped for this va
               //See if we are going to Map the same page again with same
               //perm 
               if(page2pa(pp) != PTE_ADDR( *ptr) ) 
               {
                  page_remove(pgdir,va); 
                  retval = mapva2pa(pgdir,pp,va,perm | PTE_P);
               } 
               else
               {
                   //We are given the same physical page which is 
                   //already mapped to given VA. So return silently.
                   // It is possible that , we want to re-map the same
                   // va to same PA ,so check , if we need to udpate the 
                   // permission
                   *ptr = *ptr & ~ 0xFFF;
                   *ptr = *ptr | perm;
                   *ptr = *ptr | PTE_P;
                  //cprintf("MANI-DEBUG - we are asked to map a va to the same page again - Just pretend done\n");
                   retval = 0;
                   tlb_invalidate(pgdir,va);
               }

           }
           else
           {
                  retval = mapva2pa(pgdir,pp,va,perm | PTE_P);
           }
       }
       else 
       {
             //this va has no mapping to any PA.Go ahead and map it 
             //with new PA provided.
            retval = mapva2pa(pgdir,pp,va,perm | PTE_P);
       }
         
	return  retval;
}

// This function creates a Page Table(if needed) and then updates the
// pgdir and PTE entry to map given va to PA
// PRE-CONDITION : Before calling this function , we need to
//                 Ensure that this va is NOT mapped to any PA
//                 As this function , blindly updates the PTE for a VA
int
mapva2pa(pde_t *pgdir, struct Page *pp, void *va, int perm) 
{

       pte_t * ptr = NULL;
       pte_t pte_entry = 0x00;
       int retval = -E_NO_MEM;

       //Now map the va to PA .
       ptr =  pgdir_walk(pgdir,va,1);

       //cprintf("MANI mapva2pa request to map %x to %d\n",va,pp-pages);
       //cprintf("MANI mapva2pa walk returned %x\n",ptr);


      if( ptr == NULL )
      {
          //cprintf("MANI - CRTICAL CRITICAL - WALK FAILED to create a Page tbale \n");
           retval = -E_NO_MEM;
      }
      else
      {

          //Now the last thing , update PTE Entry provided by WALK and 
          // Increase the reference count for the new page
       
           pte_entry = page2pa(pp) >> PGSHIFT;
           pte_entry = pte_entry << PGSHIFT;
           pte_entry |= perm ;
           pte_entry |= PTE_P;

           *ptr = pte_entry;

            //cprintf("Mani pte entry looks as %x\n",*ptr);

            pp->pp_ref++;
            retval = 0;

             tlb_invalidate(pgdir,va);
      }

      return( retval);
}
//
// Map [la, la+size) of linear address space to physical [pa, pa+size)
// in the page table rooted at pgdir.  Size is a multiple of PGSIZE.
// Use permission bits perm|PTE_P for the entries.
//
// This function is only intended to set up the ``static'' mappings
// above UTOP. As such, it should *not* change the pp_ref field on the
// mapped pages.
//
// Hint: the TA solution uses pgdir_walk

static void
boot_map_segment(pde_t *pgdir, uintptr_t la, size_t size, physaddr_t pa, int perm)
{
      pte_t *ptep;
      int i;

     for (i = 0; i < size; i += PGSIZE) 
     {
        //Make sure to create a Page Table , if it doesnot exist
        ptep = pgdir_walk(pgdir, (void *)(la + i), 1);

        if (ptep == NULL)
        {
                cprintf(" MANI - Critical - No Memory to create a Page Table\n");
                return;
        }

         *ptep = (pa + i) | perm | PTE_P;
    }
}

//
// Return the page mapped at virtual address 'va'.
// If pte_store is not zero, then we store in it the address
// of the pte for this page.  This is used by page_remove and
// can be used to verify page permissions for syscall arguments,
// but should not be used by most callers.
//
// Return NULL if there is no page mapped at va.
//
// Hint: the TA solution uses pgdir_walk and pa2page.
//
struct Page *
page_lookup(pde_t *pgdir, void *va, pte_t **pte_store)
{

        struct Page * pptr = NULL;

        //Given the va , lets get the corresponding PA .
        pte_t * ptr = pgdir_walk(pgdir,va,0);

        if( pte_store && *pte_store)
        {
             *pte_store = ptr;
        }
        
        if( ptr != NULL )
        {
          //WE have a Page Table , now check, if the PTE is valid
          if(( *ptr & PTE_P ) == 1 )
          {
               physaddr_t address =  PTE_ADDR( *ptr ) ;
               pptr = pa2page ( address );
          }
        }

	return pptr;
}

//
// Unmaps the physical page at virtual address 'va'.
// If there is no physical page at that address, silently does nothing.
//
// Details:
//   - The ref count on the physical page should decrement.
//   - The physical page should be freed if the refcount reaches 0.
//   - The pg table entry corresponding to 'va' should be set to 0.
//     (if such a PTE exists)
//   - The TLB must be invalidated if you remove an entry from
//     the pg dir/pg table.
//
// Hint: The TA solution is implemented using page_lookup,
// 	tlb_invalidate, and page_decref.
//
void
page_remove(pde_t *pgdir, void *va)
{
       pte_t * ptep = pgdir_walk(pgdir,va,0);

       if( ptep != NULL )
       {
            //Check if the PTE entry is valid 
            if ( ( *ptep & PTE_P ) == 1 )
            {
               //The given va is already mapped at a PA 
               physaddr_t page_pa = PTE_ADDR( *ptep ); 
               struct Page * pp = pa2page( page_pa );
               if ( pp != NULL )
               {
                   //Indicate that one reference is reduced for this 
                   // Physical page
                   page_decref( pp );

                    //Now unset the PRESENT BIT 
                    *ptep = *ptep & ~PTE_P ;

                   tlb_invalidate(pgdir,va);
               }
            }
       }
}

//
// Invalidate a TLB entry, but only if the page tables being
// edited are the ones currently in use by the processor.
//
void
tlb_invalidate(pde_t *pgdir, void *va)
{
	// Flush the entry only if we're modifying the current address space.
	if (!curenv || curenv->env_pgdir == pgdir)
		invlpg(va);
}

static uintptr_t user_mem_check_addr;

//
// Check that an environment is allowed to access the range of memory
// [va, va+len) with permissions 'perm | PTE_P'.
// Normally 'perm' will contain PTE_U at least, but this is not required.
// 'va' and 'len' need not be page-aligned; you must test every page that
// contains any of that range.  You will test either 'len/PGSIZE',
// 'len/PGSIZE + 1', or 'len/PGSIZE + 2' pages.
//
// A user program can access a virtual address if (1) the address is below
// ULIM, and (2) the page table gives it permission.  These are exactly
// the tests you should implement here.
//
// If there is an error, set the 'user_mem_check_addr' variable to the first
// erroneous virtual address.
//
// Returns 0 if the user program can access this range of addresses,
// and -E_FAULT otherwise.
//
int
user_mem_check(struct Env *env, const void *va, size_t len, int perm)
{
        uint32_t va_down = ROUNDDOWN((uint32_t)va,PGSIZE);
        uint32_t len_up  = ROUNDUP(len,PGSIZE);
         //TODO : len_up is INCOORECT
	// LAB 3: Your code here. 
        uint32_t addr = va_down; 
        pte_t *ptep = 0x00;

        //set user_mem_check_addr to o
        user_mem_check_addr = 0x00;


        //cprintf("user_mem_check addr %x len %d \n",va_down,len_up);
        //cprintf("user_mem_check addr %x len %d env %x\n",va,len,env->env_id);
        //dump_directory_content(env->env_pgdir);
        for( ; addr < (va_down + len_up);addr += PGSIZE)
        {
            //cprintf("Checking for addr %x %x\n",addr,env->env_pgdir);
            ptep = pgdir_walk(env->env_pgdir,(void*)addr,0);
            //cprintf("Address - %x  ptep %x con %x\n",(void*)addr,ptep,*ptep);

            if(( ptep == NULL )       ||
               ((*ptep & PTE_P) == 0)  ||
               ((*ptep & perm) == 0 )
             ) 
             {
                  //found an entry , where permission s not set 
                  if( addr < (uintptr_t)va )
                  {
                     user_mem_check_addr = (uintptr_t)va;
                  }
                  else
                  {
                     user_mem_check_addr = (uintptr_t)addr;
                  }
                  return -E_FAULT;
             }

     } 
	return 0;
}

//
// Checks that environment 'env' is allowed to access the range
// of memory [va, va+len) with permissions 'perm | PTE_U'.
// If it can, then the function simply returns.
// If it cannot, 'env' is destroyed and, if env is the current
// environment, this function will not return.
//
void
user_mem_assert(struct Env *env, const void *va, size_t len, int perm)
{
	if (user_mem_check(env, va, len, perm | PTE_U) < 0) {
		cprintf("[%08x] user_mem_check assertion failure for "
			"va %08x\n", env->env_id, user_mem_check_addr);
		env_destroy(env);	// may not return
	}
}

// check page_insert, page_remove, &c
static void
page_check(void)
{
	struct Page *pp, *pp0, *pp1, *pp2;
	struct Page_list fl;
	pte_t *ptep, *ptep1;
	void *va;
	int i;

	// should be able to allocate three pages
	pp0 = pp1 = pp2 = 0;
	assert(page_alloc(&pp0) == 0);
	assert(page_alloc(&pp1) == 0);
	assert(page_alloc(&pp2) == 0);

	assert(pp0);
	assert(pp1 && pp1 != pp0);
	assert(pp2 && pp2 != pp1 && pp2 != pp0);

	// temporarily steal the rest of the free pages
	fl = page_free_list;
	LIST_INIT(&page_free_list);

	// should be no free memory
	assert(page_alloc(&pp) == -E_NO_MEM);

	// there is no page allocated at address 0
	assert(page_lookup(boot_pgdir, (void *) 0x0, &ptep) == NULL);

	// there is no free memory, so we can't allocate a page table 
	assert(page_insert(boot_pgdir, pp1, 0x0, 0) < 0);

	// free pp0 and try again: pp0 should be used for page table
	page_free(pp0);
	assert(page_insert(boot_pgdir, pp1, 0x0, 0) == 0);
	assert(PTE_ADDR(boot_pgdir[0]) == page2pa(pp0));
	assert(check_va2pa(boot_pgdir, 0x0) == page2pa(pp1));
	assert(pp1->pp_ref == 1);
	assert(pp0->pp_ref == 1);

	// should be able to map pp2 at PGSIZE because pp0 is already allocated for page table
	assert(page_insert(boot_pgdir, pp2, (void*) PGSIZE, 0) == 0);
	assert(check_va2pa(boot_pgdir, PGSIZE) == page2pa(pp2));
	assert(pp2->pp_ref == 1);

	// should be no free memory
	assert(page_alloc(&pp) == -E_NO_MEM);

	// should be able to map pp2 at PGSIZE because it's already there
	assert(page_insert(boot_pgdir, pp2, (void*) PGSIZE, 0) == 0);
	assert(check_va2pa(boot_pgdir, PGSIZE) == page2pa(pp2));
	assert(pp2->pp_ref == 1);

	// pp2 should NOT be on the free list
	// could happen in ref counts are handled sloppily in page_insert
	assert(page_alloc(&pp) == -E_NO_MEM);

	// check that pgdir_walk returns a pointer to the pte
	ptep = KADDR(PTE_ADDR(boot_pgdir[PDX(PGSIZE)]));
	assert(pgdir_walk(boot_pgdir, (void*)PGSIZE, 0) == ptep+PTX(PGSIZE));

	// should be able to change permissions too.
	assert(page_insert(boot_pgdir, pp2, (void*) PGSIZE, PTE_U) == 0);
	assert(check_va2pa(boot_pgdir, PGSIZE) == page2pa(pp2));
	assert(pp2->pp_ref == 1);
	assert(*pgdir_walk(boot_pgdir, (void*) PGSIZE, 0) & PTE_U);
	assert(boot_pgdir[0] & PTE_U);
	
	// should not be able to map at PTSIZE because need free page for page table
	assert(page_insert(boot_pgdir, pp0, (void*) PTSIZE, 0) < 0);

	// insert pp1 at PGSIZE (replacing pp2)
	assert(page_insert(boot_pgdir, pp1, (void*) PGSIZE, 0) == 0);
	assert(!(*pgdir_walk(boot_pgdir, (void*) PGSIZE, 0) & PTE_U));

	// should have pp1 at both 0 and PGSIZE, pp2 nowhere, ...
	assert(check_va2pa(boot_pgdir, 0) == page2pa(pp1));
	assert(check_va2pa(boot_pgdir, PGSIZE) == page2pa(pp1));
	// ... and ref counts should reflect this
	assert(pp1->pp_ref == 2);
	assert(pp2->pp_ref == 0);

	// pp2 should be returned by page_alloc
	assert(page_alloc(&pp) == 0 && pp == pp2);

	// unmapping pp1 at 0 should keep pp1 at PGSIZE
	page_remove(boot_pgdir, 0x0);
	assert(check_va2pa(boot_pgdir, 0x0) == ~0);
	assert(check_va2pa(boot_pgdir, PGSIZE) == page2pa(pp1));
	assert(pp1->pp_ref == 1);
	assert(pp2->pp_ref == 0);

	// unmapping pp1 at PGSIZE should free it
	page_remove(boot_pgdir, (void*) PGSIZE);
	assert(check_va2pa(boot_pgdir, 0x0) == ~0);
	assert(check_va2pa(boot_pgdir, PGSIZE) == ~0);
	assert(pp1->pp_ref == 0);
	assert(pp2->pp_ref == 0);

	// so it should be returned by page_alloc
	assert(page_alloc(&pp) == 0 && pp == pp1);

	// should be no free memory
	assert(page_alloc(&pp) == -E_NO_MEM);
	
#if 0
	// should be able to page_insert to change a page
	// and see the new data immediately.
	memset(page2kva(pp1), 1, PGSIZE);
	memset(page2kva(pp2), 2, PGSIZE);
	page_insert(boot_pgdir, pp1, 0x0, 0);
	assert(pp1->pp_ref == 1);
	assert(*(int*)0 == 0x01010101);
	page_insert(boot_pgdir, pp2, 0x0, 0);
	assert(*(int*)0 == 0x02020202);
	assert(pp2->pp_ref == 1);
	assert(pp1->pp_ref == 0);
	page_remove(boot_pgdir, 0x0);
	assert(pp2->pp_ref == 0);
#endif

	// forcibly take pp0 back
	assert(PTE_ADDR(boot_pgdir[0]) == page2pa(pp0));
	boot_pgdir[0] = 0;
	assert(pp0->pp_ref == 1);
	pp0->pp_ref = 0;
	
	// check pointer arithmetic in pgdir_walk
	page_free(pp0);
	va = (void*)(PGSIZE * NPDENTRIES + PGSIZE);
	ptep = pgdir_walk(boot_pgdir, va, 1);
	ptep1 = KADDR(PTE_ADDR(boot_pgdir[PDX(va)]));
	assert(ptep == ptep1 + PTX(va));
	boot_pgdir[PDX(va)] = 0;
	pp0->pp_ref = 0;
	
	// check that new page tables get cleared
	memset(page2kva(pp0), 0xFF, PGSIZE);
	page_free(pp0);
	pgdir_walk(boot_pgdir, 0x0, 1);
	ptep = page2kva(pp0);
	for(i=0; i<NPTENTRIES; i++)
		assert((ptep[i] & PTE_P) == 0);
	boot_pgdir[0] = 0;
	pp0->pp_ref = 0;

	// give free list back
	page_free_list = fl;

	// free the pages we took
	page_free(pp0);
	page_free(pp1);
	page_free(pp2);
	
	cprintf("page_check() succeeded!\n");
}

